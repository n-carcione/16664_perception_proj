{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/n-carcione/16664_perception_proj/blob/main/proj_script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy-XY3W7JiPK",
        "outputId": "568f6296-8a7c-403d-c6d7-5ead95abb76b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#@title mount your Google Drive\n",
        "#@markdown Your work will be stored in a folder called `hw_16831` by default to prevent Colab instance timeouts from deleting your edits.\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8vrTt3tQKoQr"
      },
      "outputs": [],
      "source": [
        "#@title set up mount symlink\n",
        "\n",
        "DRIVE_PATH = '/content/gdrive/My\\ Drive/16664_percep_proj'\n",
        "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
        "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
        "  %mkdir $DRIVE_PATH\n",
        "\n",
        "## the space in `My Drive` causes some issues,\n",
        "## make a symlink to avoid this\n",
        "SYM_PATH = '/content/16664_percep_proj'\n",
        "if not os.path.exists(SYM_PATH):\n",
        "  !ln -s $DRIVE_PATH $SYM_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbqeNRXlLBXv",
        "outputId": "44c04e7c-2921-4af2-d560-da7006456234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/16664_percep_proj\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n",
            "Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:11 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "25 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ]
        }
      ],
      "source": [
        "%cd /content/gdrive/My Drive/16664_percep_proj\n",
        "!apt update \n",
        "!apt install -y --no-install-recommends \\\n",
        "        build-essential \\\n",
        "        curl \\\n",
        "        git \\\n",
        "        gnupg2 \\\n",
        "        make \\\n",
        "        cmake \\\n",
        "        ffmpeg \\\n",
        "        swig \\\n",
        "        libz-dev \\\n",
        "        unzip \\\n",
        "        zlib1g-dev \\\n",
        "        libglfw3 \\\n",
        "        libglfw3-dev \\\n",
        "        libxrandr2 \\\n",
        "        libxinerama-dev \\\n",
        "        libxi6 \\\n",
        "        libxcursor-dev \\\n",
        "        libgl1-mesa-dev \\\n",
        "        libgl1-mesa-glx \\\n",
        "        libglew-dev \\\n",
        "        libosmesa6-dev \\\n",
        "        lsb-release \\\n",
        "        ack-grep \\\n",
        "        patchelf \\\n",
        "        wget \\\n",
        "        xpra \\\n",
        "        xserver-xorg-dev \\\n",
        "        xvfb \\\n",
        "        python-opengl \\\n",
        "        ffmpeg > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2TApiZSLiwD",
        "outputId": "b1b09f51-46c2-401d-9b92-23e40b570989"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement csv (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for csv\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.0.0+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchvision) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchvision) (16.0.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->torchvision) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->torchvision) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install pandas\n",
        "%pip install matplotlib\n",
        "%pip install numpy\n",
        "%pip install csv\n",
        "%pip install torch\n",
        "%pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7az-iZmfMKxM",
        "outputId": "c64ffc43-d199-452b-e0bc-744a11e499b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/16664_percep_proj\n"
          ]
        }
      ],
      "source": [
        "%cd /content/gdrive/My Drive/16664_percep_proj\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.annotations = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index,0]+\"_image.jpg\")\n",
        "        image = plt.imread(img_path)\n",
        "        y_label = torch.tensor(int(self.annotations.iloc[index,1]))\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return(image, y_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QiZyXSRhMT34"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Example architecture\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        # Define the network layer structure here\n",
        "        # Example from the website shown here\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=5, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(12)\n",
        "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=5, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(12)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=5, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(24)\n",
        "        self.conv4 = nn.Conv2d(in_channels=24, out_channels=24, kernel_size=5, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(24)\n",
        "        self.fc1 = nn.Linear(24*520*951, 3)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Define how a forward pass through the network would look here\n",
        "        # This function contains all the nonlinear functions between layers\n",
        "        # The example from the website is continued here\n",
        "        output = F.relu(self.bn1(self.conv1(input)))\n",
        "        output = F.relu(self.bn2(self.conv2(output)))\n",
        "        output = self.pool(output)\n",
        "        output = F.relu(self.bn3(self.conv3(output)))\n",
        "        output = F.relu(self.bn4(self.conv4(output)))\n",
        "        output = output.view(-1, 24*520*951)\n",
        "        output = self.fc1(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# # Architecture from the slides\n",
        "# class Network(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Network, self).__init__()\n",
        "#         # Define the network layer structure here\n",
        "#         # Example from the slides shown here\n",
        "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=5, stride=1, padding=1)\n",
        "#         self.bn1 = nn.BatchNorm2d(12)\n",
        "#         self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=5, stride=1, padding=1)\n",
        "#         self.bn2 = nn.BatchNorm2d(12)\n",
        "#         self.pool1 = nn.MaxPool2d(2,2)\n",
        "#         self.conv3 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=5, stride=1, padding=1)\n",
        "#         self.bn3 = nn.BatchNorm2d(24)\n",
        "#         self.conv4 = nn.Conv2d(in_channels=24, out_channels=24, kernel_size=5, stride=1, padding=1)\n",
        "#         self.bn4 = nn.BatchNorm2d(24)\n",
        "#         self.pool2 = nn.MaxPool2d(2,2)\n",
        "#         self.conv5 = nn.Conv2d(in_channels=24, out_channels=24, kernel_size=5, stride=1, padding=1)\n",
        "#         self.bn5 = nn.BatchNorm2d(24)\n",
        "#         self.conv6 = nn.Conv2d(in_channels=24, out_channels=24, kernel_size=5, stride=1, padding=1)\n",
        "#         self.bn6 = nn.BatchNorm2d(24)\n",
        "#         self.pool3 = nn.MaxPool2d(2,2)\n",
        "#         self.fc1 = nn.Linear(24*128*235, 3)\n",
        "\n",
        "#     def forward(self, input):\n",
        "#         # Define how a forward pass through the network would look here\n",
        "#         # This function contains all the nonlinear functions between layers\n",
        "#         output = F.relu(self.bn1(self.conv1(input)))\n",
        "#         output = F.relu(self.bn2(self.conv2(output)))\n",
        "#         output = self.pool1(output)\n",
        "#         output = F.relu(self.bn3(self.conv3(output)))\n",
        "#         output = F.relu(self.bn4(self.conv4(output)))\n",
        "#         output = self.pool2(output)\n",
        "#         output = F.relu(self.bn5(self.conv5(output)))\n",
        "#         output = F.relu(self.bn6(self.conv6(output)))\n",
        "#         output = self.pool3(output)\n",
        "#         output = output.view(-1, 24*128*235)\n",
        "#         output = self.fc1(output)\n",
        "\n",
        "#         return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tjuqkw0aMZQN",
        "outputId": "a3446acc-fab6-40f5-dae5-dea5e2a056e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/16664_percep_proj\n",
            "The model willbe running on  cuda:0  device\n",
            "[1,   100] loss: 63.445\n",
            "Saving model!!\n",
            "[1,   200] loss: 23.141\n",
            "Saving model!!\n",
            "[1,   300] loss: 16.958\n",
            "Saving model!!\n",
            "[1,   400] loss: 11.735\n",
            "Saving model!!\n",
            "[1,   500] loss: 6.509\n",
            "Saving model!!\n",
            "[1,   600] loss: 4.898\n",
            "Saving model!!\n",
            "[1,   700] loss: 2.793\n",
            "Saving model!!\n",
            "[2,   100] loss: 1.960\n",
            "Saving model!!\n",
            "[2,   200] loss: 1.623\n",
            "Saving model!!\n",
            "[2,   300] loss: 2.040\n",
            "Saving model!!\n",
            "[2,   400] loss: 1.432\n",
            "Saving model!!\n",
            "[2,   500] loss: 1.523\n",
            "Saving model!!\n",
            "[2,   600] loss: 1.159\n",
            "Saving model!!\n",
            "[2,   700] loss: 1.018\n",
            "Saving model!!\n",
            "[3,   100] loss: 0.719\n",
            "Saving model!!\n",
            "[3,   200] loss: 0.651\n",
            "Saving model!!\n",
            "[3,   300] loss: 0.708\n",
            "Saving model!!\n",
            "[3,   400] loss: 0.828\n",
            "Saving model!!\n",
            "[3,   500] loss: 0.701\n",
            "Saving model!!\n",
            "[3,   600] loss: 0.605\n",
            "Saving model!!\n",
            "[3,   700] loss: 0.776\n",
            "Saving model!!\n",
            "[4,   100] loss: 0.635\n",
            "Saving model!!\n",
            "[4,   200] loss: 0.536\n",
            "Saving model!!\n",
            "[4,   300] loss: 0.491\n",
            "Saving model!!\n",
            "[4,   400] loss: 0.495\n",
            "Saving model!!\n",
            "[4,   500] loss: 0.532\n",
            "Saving model!!\n",
            "[4,   600] loss: 0.554\n",
            "Saving model!!\n",
            "[4,   700] loss: 0.491\n",
            "Saving model!!\n",
            "[5,   100] loss: 0.439\n",
            "Saving model!!\n",
            "[5,   200] loss: 0.457\n",
            "Saving model!!\n",
            "[5,   300] loss: 0.417\n",
            "Saving model!!\n",
            "[5,   400] loss: 0.446\n",
            "Saving model!!\n",
            "[5,   500] loss: 0.482\n",
            "Saving model!!\n",
            "[5,   600] loss: 0.673\n",
            "Saving model!!\n",
            "[5,   700] loss: 0.527\n",
            "Saving model!!\n",
            "Finished training\n",
            "Saving model!!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 10\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "\n",
        "%cd /content/gdrive/My Drive/16664_percep_proj\n",
        "csv_file = os.path.join(os.getcwd(), \"trainval/labels.csv\")\n",
        "root_dir = os.path.join(os.getcwd(), \"trainval\")\n",
        "transformations = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "training_data = CustomDataset(csv_file=csv_file, root_dir=root_dir, transform=transformations)\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = Network()\n",
        "# path = os.path.join(os.getcwd(), \"trained_model.pth\")\n",
        "# model.load_state_dict(torch.load(path))\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "def saveModel():\n",
        "    print(\"Saving model!!\")\n",
        "    path = os.path.join(os.getcwd(), \"./trained_model.pth\")\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "def train(num_epochs):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"The model willbe running on \", device, \" device\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i, (images, labels) in enumerate(train_dataloader, 0):\n",
        "            # get the inputs\n",
        "            images = Variable(images.to(device))\n",
        "            labels = Variable(labels.to(device))\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # predict the classes using images from the training set\n",
        "            outputs = model(images)\n",
        "            # compute the loss based on model output and real labels\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            # backprop the loss\n",
        "            loss.backward()\n",
        "            # adjust the parameters based on the calculated gradients\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 100 == 99:\n",
        "                print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, running_loss/100))\n",
        "                running_loss = 0.0\n",
        "                saveModel()\n",
        "\n",
        "train(5)\n",
        "print(\"Finished training\")\n",
        "saveModel()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import transforms\n",
        "from glob import glob\n",
        "import csv\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Loading the model trained above\n",
        "model = Network()\n",
        "path = os.path.join(os.getcwd(), \"trained_model.pth\")\n",
        "model.load_state_dict(torch.load(path))\n",
        "transformations = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Get all the test image file locations\n",
        "%cd /content/gdrive/My Drive/16664_percep_proj\n",
        "sub_csv_file = os.path.join(os.getcwd(), \"submission.csv\")\n",
        "root_dir = os.path.join(os.getcwd(), \"test\")\n",
        "test_files = glob(os.path.join(root_dir, \"*/*_image.jpg\"))\n",
        "\n",
        "# Trim the file names to just the folder and image number to match expected csv format\n",
        "trimmed_files = [s.replace(os.getcwd(),'') for s in test_files]\n",
        "trimmed_files = [s.replace('/test/','') for s in trimmed_files]\n",
        "trimmed_files = [s.replace('_image.jpg','') for s in trimmed_files]\n",
        "\n",
        "# Pass each test file through the trained network and record the predicted label\n",
        "predictions = []\n",
        "i = 0\n",
        "for test_file in test_files:\n",
        "  img = plt.imread(test_file)\n",
        "  input = transformations(img)\n",
        "  input = input[None, :]\n",
        "  output = model(input)\n",
        "  _, predicted = torch.max(output, 1)\n",
        "  predictions.append(predicted.item())\n",
        "  if i % 50 == 0:\n",
        "    print(i)\n",
        "  i += 1\n",
        "\n",
        "# Create the csv file, including the expected header\n",
        "header = ['guid/image', 'label']\n",
        "data = [[file, pred] for (file, pred) in zip(trimmed_files, predictions)]\n",
        "with open(sub_csv_file, 'w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(header)\n",
        "    writer.writerows(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncmXfbIHPGzC",
        "outputId": "62f0d430-b7b5-4a48-ab14-3fa60cbdb727"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/16664_percep_proj\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "250\n",
            "300\n",
            "350\n",
            "400\n",
            "450\n",
            "500\n",
            "550\n",
            "600\n",
            "650\n",
            "700\n",
            "750\n",
            "800\n",
            "850\n",
            "900\n",
            "950\n",
            "1000\n",
            "1050\n",
            "1100\n",
            "1150\n",
            "1200\n",
            "1250\n",
            "1300\n",
            "1350\n",
            "1400\n",
            "1450\n",
            "1500\n",
            "1550\n",
            "1600\n",
            "1650\n",
            "1700\n",
            "1750\n",
            "1800\n",
            "1850\n",
            "1900\n",
            "1950\n",
            "2000\n",
            "2050\n",
            "2100\n",
            "2150\n",
            "2200\n",
            "2250\n",
            "2300\n",
            "2350\n",
            "2400\n",
            "2450\n",
            "2500\n",
            "2550\n",
            "2600\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNp1haRgOsonyYiX7fqQLjd",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}